

\documentclass[
thesis  % please do not delete or change this line!
]{csthes}

%\documentclass[12pt]{report}

% The following packages are loaded automatically. Do not load them
% manually as you might risk an option clash:
%   -babel (heading translation and hyphenation)
%   -natbib (citing, see www.ctan.org/tex-archive/macros/latex/contrib/natbib/natbib.pdf)
%   -fancyhdr (headers and footers)
%   -fontenc (output encoding)

% Set the encoding your tex-source is written in.
% Use utf8 if possible. Other options that might 
% work for you are:
%   latin1 (Unix/Linux or Windows)
%   ansinew (Windows)
%   applemac (Mac)
\usepackage[utf8]{inputenc}

%for tables
\usepackage[table,xcdraw]{xcolor}
\usepackage{makecell}
\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}
\renewcommand\theadfont{\bfseries}


%for graphics
\usepackage{graphicx}
\usepackage{subcaption}

% for pseudo algorithms
\usepackage{algpseudocode}
\usepackage{algorithm2e}

% For the online version enable hyperlinks
\usepackage{hyperref}

\usepackage{amsmath}

\usepackage{multirow}


\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.5}

\usepackage{rotating}

\usepackage{tabularx}
\usepackage{booktabs}

\usepackage{float}

\usepackage[numbers]{natbib}

\usepackage[margin=3.2cm]{geometry}

\usepackage[skip=10pt plus1pt, indent=30pt]{parskip}

\usepackage{listings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Set information about your thesis here  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Digital Video Stabilization \\ of an \\ Oscillating Camera \\
using an \\ Inertial Measurement Unit}
\subtitle{}% optional
\setthesistype{Master}
\author{Ibad Rather \matr{1532894}}

\degree{MS Mechatronics}
\supervisor{\\ University: Prof. Dr. Bhaskar Choubey \\Company: Dr. Steffen Urban 
            }% most likely

% Start of content
\begin{document}
% First set the title page
\maketitle
% Give a short summary of your thesis
\begin{abstract}
Video stabilization is ubiquitous in today's smartphones and action cameras. However, instead of classical post-processing image stabilization where image features are extracted and tracked over time \citep{morimoto1998evaluation} or modern deep learning based methods using dense optical flow \citep{yu2020learning}, they need to perform the motion estimation on constrained hardware systems and with very high  frame-rates of up to 100 fps or more. 

Hence the motion estimation, that is necessary to correct vibrations or shaking is performed using Inertial Measurement Units (IMUs). Typically, the motion estimates are reduced to rotational motions \citep{karpenko2011digital} (using gyroscopes) as the distance between frames is small and the field-of-view of the lenses large $(\geq 50^\circ)$. In addition, estimating translational motion is a difficult challenge using low-cost IMUs as the double integration of accelerometer readings is prone to strong drift and leads to large errors even over short time periods $(\leq 1s)$. 

This thesis investigates and develops a video stabilization pipeline for an oscillating camera using IMUs. The camera has a small field-of-view and hence small translational movements along the viewing direction will be visible and need to be stabilized. The camera is coupled with an IMU and attached to a linear stage, that will mimic the oscillating movements.

\end{abstract}
% Set the table of contents
\tableofcontents%

% And other tables/lists optionally
\listoftables%
\listoffigures%

% Start the main work (handles page numbering etc.)
\mainmatter

% While writing your thesis it is advisable to only load the chapter you are currently working on. It decreases the compile time. 

% 1. Chapter: ''Introduction''
\input{chapters/chapter_1}
% 2. Chapter: ''Fundamentals Blocks''
\input{chapters/chapter_2}
% 3. Chapter: ''State of Research''
\input{chapters/chapter_3}
% 4. Chapter: ''Methods''
\input{chapters/chapter_4}
% 5. Chapter: ''Realisation and Evaluation''
\input{chapters/chapter_5}

% The chapters Introduction and Conclusions have NO sections
% For the other chapters:
% Each sectioning (chapter, section, subsection, ...) should have NONE
% or AT LEAST TWO sub parts!
%\section{First Section}
%\subsection{First Subsection}
%\subsubsection{First Subsubsection}
%\paragraph{First Paragraph.} Some content for the first paragraph. 
%\subparagraph{First Subparagraph.} Some content for the first subparagraph. 
% Please note the dot after the paragraph and subparagraph heading. This is not accidently.

%Use bibtex for references to literature. 
%The most references will be in the second chapter.

% Make the references section
\bibliography{references} % According to the name of your bib file

% Start the appendix
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Place appendix content here            %
%  or use \input{appendix/appendixname}   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Neural Network code in PyTorch}

\section{Model Summary}

\section{1D Convolutional Neural Network}
\begin{lstlisting}
import torch.nn as nn

class CNN1D(nn.Module):
    def __init__(
        self,
        n_features,
        n_targets,
        window_size,
        num_grid_rows,
        out_channels=32,
        kernel_size=5,
        stride=1,
        dropout=0.1,
        activation="relu",
    ):
        super(CNN1D, self).__init__()

        self.n_features = n_features
        self.n_targets = n_targets
        self.out_channels = out_channels
        self.dropout = dropout
        self.num_grid_rows = num_grid_rows
        self.window_size = window_size

        self.output_dim = n_targets

        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "leaky_relu":
            self.activation = nn.LeakyReLU(inplace=False)

        self.conv1 = nn.Sequential(
            nn.Conv1d(
                self.n_features,
                self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(self.out_channels),
            self.activation,
        )

        self.conv2 = nn.Sequential(
            nn.Conv1d(
                self.out_channels,
                2 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(2 * self.out_channels),
            self.activation,
        )

        self.conv3 = nn.Sequential(
            nn.Conv1d(
                2 * self.out_channels,
                2 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(2 * self.out_channels),
            self.activation,
        )

        self.conv4 = nn.Sequential(
            nn.Conv1d(
                2 * self.out_channels,
                4 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(4 * self.out_channels),
            self.activation,
        )

        self.conv5 = nn.Sequential(
            nn.Conv1d(
                4 * self.out_channels,
                4 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(4 * self.out_channels),
            self.activation,
        )

        self.conv6 = nn.Sequential(
            nn.Conv1d(
                4 * self.out_channels,
                4 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(4 * self.out_channels),
            self.activation,
        )

        self.conv7 = nn.Sequential(
            nn.Conv1d(
                4 * self.out_channels,
                8 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(8 * self.out_channels),
            self.activation,
        )

        self.conv8 = nn.Sequential(
            nn.Conv1d(
                8 * self.out_channels,
                8 * self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
            ),
            nn.BatchNorm1d(8 * self.out_channels),
            self.activation,
        )

        self.dropout = nn.Dropout(dropout, inplace=False)

        self.maxpool = nn.MaxPool1d(2)

        self.fc1 = nn.Sequential(
            nn.Linear(
            8 * self.out_channels, self.window_size, bias=True),
            self.activation,
        )

        self.fc2 = nn.Sequential(
            nn.Linear(
            self.window_size, self.window_size, bias=True),
            self.activation
        )

        self.output = nn.Linear(self.window_size, self.output_dim,
                                bias=True)

    def _initialize(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.xavier_normal_(m.weight, 0.1)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        out = x

        # Convolutions
        out = self.conv1(out)
        out = self.conv2(out)
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.conv5(out)
        out = self.conv6(out)
        out = self.conv7(out)
        out = self.conv8(out)

        # Dropout
        out = self.dropout(out)

        # Maxpooling
        out = self.maxpool(out)

        out = out.mean(-1)

        # Changing Dimensions to fit into fc1 layer
        out = out.view(out.shape[0], -1)

        # Fully Connected Layers
        out = self.fc1(out)
        out = self.fc2(out)

        # Output
        output = self.output(out)

        return output
\end{lstlisting}

\section{ResNet}

\begin{lstlisting}
    import torch.nn as nn


def conv3(in_planes, out_planes, kernel_size, stride=1, dilation=1):
    return nn.Conv1d(
        in_planes,
        out_planes,
        kernel_size=kernel_size,
        stride=stride,
        padding=kernel_size // 2,
        bias=False,
    )


class BasicBlock1D(nn.Module):
    expansion = 1

    def __init__(
        self, in_planes, out_planes, 
        kernel_size, stride=1, dilation=1, downsample=None
    ):
        super(BasicBlock1D, self).__init__()
        self.conv1 = conv3(in_planes, out_planes, kernel_size, stride, dilation)
        self.bn1 = nn.BatchNorm1d(out_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3(out_planes, out_planes, kernel_size)
        self.bn2 = nn.BatchNorm1d(out_planes)
        self.stride = stride
        self.downsample = downsample

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck1D(nn.Module):
    expansion = 4

    def __init__(
        self, in_planes, out_planes, kernel_size, stride=1, dilation=1, downsample=None
    ):
        super(Bottleneck1D, self).__init__()
        self.conv1 = nn.Conv1d(in_planes, out_planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm1d(out_planes)
        self.conv2 = conv3(out_planes, out_planes, kernel_size, stride, dilation)
        self.bn2 = nn.BatchNorm1d(out_planes)
        self.conv3 = nn.Conv1d(
            out_planes, out_planes * self.expansion, kernel_size=1, bias=False
        )
        self.bn3 = nn.BatchNorm1d(out_planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.stride = stride
        self.downsample = downsample

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet1D(nn.Module):
    def __init__(
        self,
        n_features,
        n_targets,
        block_type=BasicBlock1D,
        group_sizes=[2, 2, 2, 2],
        base_plane=64,
        zero_init_residual=False,
        num_grid_rows=10,
        window_size=150,
        **kwargs
    ):
        super(ResNet1D, self).__init__()
        self.base_plane = base_plane
        self.inplanes = self.base_plane

        self.activation = nn.ReLU()

        self.n_targets = n_targets
        self.num_grid_rows = num_grid_rows
        self.window_size = window_size
        self.output_dim = self.n_targets * self.num_grid_rows

        # Input module
        self.input_block = nn.Sequential(
            nn.Conv1d(
                n_features,
                self.inplanes,
                kernel_size=7,
                stride=2,
                padding=3,
                bias=False,
            ),
            nn.BatchNorm1d(self.inplanes),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),
        )

        # Residual groups
        self.planes = [self.base_plane * (2 ** i) for i in range(len(group_sizes))]
        kernel_size = kwargs.get("kernel_size", 3)
        strides = [1] + [2] * (len(group_sizes) - 1)
        dilations = [1] * len(group_sizes)
        groups = [
            self._make_residual_group1d(
                block_type,
                self.planes[i],
                kernel_size,
                group_sizes[i],
                strides[i],
                dilations[i],
            )
            for i in range(len(group_sizes))
        ]
        self.residual_groups = nn.Sequential(*groups)

        # Output
        self.output_block = nn.Sequential(
            nn.Linear(512, self.window_size, bias=True),
            self.activation,
            nn.Linear(self.window_size, self.window_size, bias=True),
            self.activation,
            nn.Linear(self.window_size, self.output_dim, bias=True),
        )

        self._initialize(zero_init_residual)

    def _make_residual_group1d(
        self, block_type, planes, kernel_size, blocks, stride=1, dilation=1
    ):
        downsample = None
        if stride != 1 or self.inplanes != planes * block_type.expansion:
            downsample = nn.Sequential(
                nn.Conv1d(
                    self.inplanes,
                    planes * block_type.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm1d(planes * block_type.expansion),
            )
        layers = []
        layers.append(
            block_type(
                self.inplanes,
                planes,
                kernel_size=kernel_size,
                stride=stride,
                dilation=dilation,
                downsample=downsample,
            )
        )
        self.inplanes = planes * block_type.expansion
        for _ in range(1, blocks):
            layers.append(block_type(self.inplanes, planes, kernel_size=kernel_size))

        return nn.Sequential(*layers)

    def _initialize(self, zero_init_residual):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck1D):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock1D):
                    nn.init.constant_(m.bn2.weight, 0)

    def forward(self, x):
        x = self.input_block(x)
        x = self.residual_groups(x)

        # Reshape to fit FC layers
        x = x.mean(-1)
        x = x.view(x.shape[0], -1)
        x = self.output_block(x)

        return x


\end{lstlisting}

\subsection{First Appendix Subsection}
\subsubsection{First Appendix Subsubsection}

\cleardoublepage

%----------------------------------------------------------------------------------------
%	DECLARATION PAGE
%---------------------------------------------------------------------------------------

% TODO uncomment the next three lines if your thesis is wirtten in german 
% \chapter*{Eidesstattliche Erklärung}
% \markboth{Eidesstattliche Erklärung}{Eidesstattliche Erklärung}
% \addcontentsline{toc}{chapter}{Eidesstattliche Erklärung}

% TODO comment the next three lines if your thesis is written in german 
\chapter*{Declaration of Authorship}
\markboth{Declaration of Authorship}{Declaration of Authorship}
\addcontentsline{toc}{chapter}{Declaration of Authorship}
% TODO Change the declaration according as needed. 

I hereby declare that I have written the above {\thesistype} thesis report independently and that I have not used any sources or aids other than those indicated. 

\bigskip

\begin{tabular}{@{}l@{}}
  Jena, Germany \rule[-0.8em]{7em}{0.5pt}\\[2ex]
  ~
\end{tabular}
\hspace{\fill}%
\begin{tabular}{@{}c@{}}
  \rule[-0.8em]{19em}{0.5pt}\\[2ex]
  {Ibad Rather}
\end{tabular}\hspace{\fill}

% Finish content and document
\end{document}
